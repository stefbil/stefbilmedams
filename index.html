<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adaptive Deesser Application - Report</title>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Merriweather:ital,wght@0,300;0,400;0,700;1,300;1,400&family=Fira+Code&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="images/favicon.png">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

    <header>
        <h1>Adaptive Deesser Application</h1>
        <div class="meta">
            <p>Stefanos Biliousis<br>
                Aalborg University, Copenhagen<br>
                Technical Faculty of IT and Design<br>
                Medialogy MSc<br>
                Sound and Music Computing Specialization<br>
                Copenhagen, Denmark<br>
                <a href="mailto:sbilio25@student.aau.dk" style="color: #aaddff;">sbilio25@student.aau.dk</a>
            </p>
        </div>
    </header>

    <nav>
        <a href="#abstract">Abstract</a>
        <a href="#introduction">Introduction</a>
        <a href="#methodology">Methodology</a>
        <a href="#implementation">Implementation</a>
        <a href="#results">Results</a>
        <a href="#discussion">Discussion</a>
        <a href="#genai">GenAI Statement</a>
        <a href="#references">References</a>
        <a href="#parameters">Parameters</a>
        <a href="#code-availability">Code Availability</a>
    </nav>

    <main>

        <section id="abstract" class="abstract">
            <h2>Abstract</h2>
            <p>High-fidelity vocal processing is frequently compromised by sibilance, a phenomenon characterized by
                stochastic high-frequency energy that presents unique dynamic range challenges. While traditional
                de-essing techniques often rely on static frequency bands, they fail to account for inter-speaker
                variability and changing dynamics. This project presents an adaptive real-time de-essing application,
                developed using the JUCE framework, which automatically detects and suppresses sibilant frequencies. The
                proposed methodology integrates a derivative-based frequency tracking algorithm to estimate the spectral
                centroid without the computational overhead of the Fast Fourier Transform (FFT). This is coupled with a
                dual-path envelope detection system and a relative threshold logic to distinguish sibilance from the
                wideband signal. Additionally, a dynamic harmonic exciter is implemented to restore high-frequency
                presence during non-sibilant periods. Objective spectral analysis confirms the system’s ability to
                selectively attenuate energy in the 6–11 kHz range while maintaining spectral transparency and
                minimizing artifacts.</p>

            <div class="keywords" style="margin-top: 1.5rem; font-size: 0.9rem; font-style: normal;">
                <strong>Keywords:</strong> Adaptive Signal Processing, De-essing, Digital Audio Effects, Harmonic
                Excitation, Real-time Application, Frequency Estimation, Vocal Processing
            </div>
        </section>

        <section id="introduction">
            <h2>I. Introduction</h2>
            <p>The engineering of high fidelity vocal processing tools remains one of the most persistent challenges in
                digital signal processing. Among the various artifacts inherent to vocal recording, sibilance, which is
                the high frequency acoustic energy generated by turbulent airflow through the narrow channel of the
                vocal tract during fricative consonant articulation, poses a unique dynamic range problem. Sibilant
                sounds, such as /s/, /z/, /ʃ/ (sh), and /tʃ/ (ch), are characterized by stochastic noise-like waveforms
                with spectral energy typically concentrated between 2 kHz and 10 kHz [4]. These frequencies often
                exhibit
                disproportionately high peak amplitudes compared to the vowel components of speech, particularly when
                subjected to heavy compression or saturation downstream in the signal chain. De-essing is a technique to
                reduce sibilant sounds in speech signals [1].</p>
            <p>The proposed de-esser is adaptive, meaning that sibilant frequencies are detected automatically [2].</p>
        </section>

        <section id="problem">
            <h2>II. Problem Statement and Hypothesis</h2>
            <p>Vocal recordings often suffer from excessive sibilance. The challenge is to maintain consistent vocal
                clarity across changing dynamics and timbres without manual equalization or multiple processors. The
                proposed application aims to deliver an adaptive vocal de-essing processor that automatically detects
                sibilant audio, suppresses the signal, and dynamically applies excitation when needed. This occurs in
                real-time.</p>
            <p>The hypothesis for this project was that since classic de-essing processors often use a static frequency
                bandwidth, an adaptive frequency tracking system is proposed that will dynamically adapt to any vocal
                input, without the excessive need of manual frequency tuning.</p>
            <p>The adaptive frequency de-esser applies significantly less gain reduction to non-sibilant high-frequency
                signals compared to a static frequency de-esser set to the same threshold and ratio.</p>
        </section>

        <section id="methodology">
            <h2>III. Methodology</h2>
            <p>To test this hypothesis, a real-time audio VST3 plugin was developed using C++ and the JUCE framework<a
                    href="#fn1" id="ref1"><sup>1</sup></a>. The methodology integrates Digital Signal Processing (DSP)
                theory regarding
                Stable Variable Filter (SVF) with software engineering patterns for real-time performance [3].</p>
            <p id="fn1" style="font-size: 0.8rem; margin-top: 0.5rem; color: #666;"><sup>1</sup> Source code available
                at: <a href="https://stefbil.github.io/stefbilmedams">https://stefbil.github.io/stefbilmedams</a></p>

            <h3>A. Signal Decomposition</h3>
            <p>The input signal \( x[n] \) is split into two parallel analysis paths:</p>
            <ul>
                <li><strong>Broadband Path:</strong> A copy of the raw input signal, representing the full spectral
                    energy of the signal.</li>
                <li><strong>Sidechain Path (Sibilance isolation):</strong> Processed signal through a digital State
                    Variable Filter configured as a Bandpass Filter.</li>
            </ul>
            <p>The SVF topology was chosen for its numerical stability and the ability to modulate frequency
                coefficients at audio rates without artifacts [3][16]. The configuration of the bandpass is:</p>
            <div class="math-display">
                $$ y_{sib}[n] = \text{SVF}_{Bandpass}(x[n], f_c, Q) $$
            </div>
            <p>where \( f_c \) is the center frequency (default at 6 kHz) and \( Q \) controls bandwidth with a default
                at 2.0. The bandpass output isolates energy in the sibilant region while rejecting low-frequency
                fundamentals and high-frequency noise.</p>

            <h3>B. Crossover Network</h3>
            <p>For the Split-Band processing, a 4th-order Linkwitz-Riley crossover (24 dB/Octave) decomposes the signal
                into low and high bands. This is implemented by cascading two 2nd-order State Variable Filter (SVF)
                stages in series. The high band signal is derived via complementary subtraction to ensure perfect
                reconstruction:</p>
            <div class="math-display">
                $$ x_{low}[n] = SVF_{LP}(SVF_{LP}(x[n], f_{split}), f_{split}) $$
                $$ x_{high}[n] = x[n] - x_{low}[n] $$
            </div>
            <p>This guarantees all-pass magnitude response and linear-phase summation, avoiding comb filtering that
                occurs when using independent high-pass filters [5]. The crossover frequency \( f_{split} \) is user
                adjustable between 3-10 kHz.</p>

            <h3>C. Dual Envelope Detection</h3>
            <p>To establish the relative threshold, the system calculates two control voltages representing the
                instantaneous loudness of different spectral regions.</p>

            <h4>1) Ballistics Model</h4>
            <div class="math-display">
                $$ e[n] = |u[n]| + \alpha[n] \cdot (e[n-1] - |u[n]|) $$
            </div>
            <p>where the coefficient \( \alpha[n] \) switches based on signal direction:</p>
            <div class="math-display">
                $$ \alpha[n] = \begin{cases} \alpha_{\mathrm{attack}}, & \text{if } |u[n]| > e[n-1] \quad
                (\text{rising}), \\ \alpha_{\mathrm{release}}, & \text{if } |u[n]| \le e[n-1] \quad (\text{falling}).
                \end{cases} $$
            </div>
            <p>The time constants are derived from user defined attack/release times (in milliseconds):</p>
            <div class="math-display">
                $$ \alpha_{attack} = e^{-1/(0.001 \cdot t_{attack} \cdot f_s)} $$
                $$ \alpha_{release} = e^{-1/(0.001 \cdot t_{release} \cdot f_s)} $$
            </div>
            <p>Fast attack prevents sibilant transients from escaping detection, while slow release avoids chattering on
                decaying phonemes.</p>

            <h4>2) Dual Path Computation</h4>
            <p>Two envelopes are calculated in parallel:</p>
            <ul>
                <li>\( E_{wide}[n] \): Envelope of unfiltered input signal \( x[n] \)</li>
                <li>\( E_{sib}[n] \): Envelope of the bandpass filtered signal \( y_{sib}[n] \)</li>
            </ul>
            <p>These are summed across all input channels to create control signals:</p>
            <div class="math-display">
                $$ E_{wide}^{\text{total}}[n] = \sum_{ch=1}^{N_{ch}} E_{wide,\,ch}[n] $$
                $$ E_{sib}^{\text{total}}[n] = \sum_{ch=1}^{N_{ch}} E_{sib,\,ch}[n] $$
            </div>

            <h3>D. Relative Threshold Detection Logic</h3>

            <h4>1) Core Algorithm</h4>
            <p>The detection decision is made by comparing the sibilant envelope to a threshold-scaled version of the
                wideband envelope:</p>
            <div class="math-display">
                $$ \text{if } E_{sib}[n] > E_{wide}[n] \cdot K_{\text{thresh}} \text{ then apply suppression.} $$
            </div>
            <p>Where \( K_{thresh} \) is derived from the user adjustable threshold parameter (in dB):</p>
            <div class="math-display">
                $$ K_{\text{thresh}} = 10^{T_{dB}/20} $$
            </div>

            <h4>2) Gain Reduction Calculation</h4>
            <p>Upon detecting sibilance, the gain reduction factor is calculated using a power low compression curve:
            </p>
            <div class="math-display">
                $$ \text{OverRatio} = \frac{E_{sib}[n]}{E_{wide}[n] \cdot K_{\text{thresh}} + \epsilon} $$
                $$ GR[n] = \text{OverRatio}^{-A} $$
            </div>
            <p>where \( A = \frac{amount}{100} \) and \( \epsilon = 10^{-9} \) prevent division by zero. This produces a
                smooth compression characteristic similar to that of optical compressors (Giannoulis et al.) [17].</p>

            <h4>3) Excitation Detection</h4>
            <p>Conversely, when sibilance is below threshold, the system calculates an excitation control signal:</p>
            <div class="math-display">
                $$ \text{if } E_{sib}[n] < E_{wide}[n] \cdot K_{\text{thresh}} $$ $$ Ratio_{lin}=\frac{E_{sib}[n]}{
                    E_{wide}[n]+\epsilon} $$ $$ Ratio_{dB}=20 \log_{10}(Ratio_{lin}) $$ $$ C_{excite}[n]=\text{clip}
                    \left( \frac{(T_{dB} + 20) - \text{Ratio}_{dB}}{12}, 0, 1 \right) $$ </div>
                    <p>This normalized control signal \( (0 \leq C_{excite} \leq 1) \) drives the harmonic exciter, with
                        higher values indicating more sibilance deficit.</p>

                    <h3>E. Adaptive Frequency Tracking</h3>
                    <h4>1) Problem Formulation</h4>
                    <p>Fixed frequency de-essers fail to account for inter-speaker variation. The dominant sibilance
                        frequency varies with vocal tract length (children: 8-12kHz, adults: 4-8kHz) [7], tongue
                        position, and dentition. An adaptive system must automatically locate the spectral centroid of
                        sibilance without FFT overhead.</p>

                    <h4>2) Derivative Based Estimation</h4>
                    <p>Classical spectral centroid calculation requires frequency-domain analysis [8]. However,
                        differentiation in the time domain emphasizes high frequencies (+6dB/octave per derivative) [9].
                        The system exploits this by calculating:</p>
                    <p>First derivative: \( \nu[n] = x[n] - x[n - 1] \)<br>
                        Second derivative: \( a[n] = v[n] - v[n - 1] \)</p>
                    <p>The ratio of their energies proportional to frequency [10]. estimate:</p>
                    <div class="math-display">
                        $$ E_v = \sum_{n=0}^{N-1} v[n]^2 $$
                        $$ E_a = \sum_{n=0}^{N-1} a[n]^2 $$
                        $$ f_{\text{inst}} = \frac{f_s}{2\pi} \cdot \sqrt{\frac{E_a}{E_v}} $$
                    </div>

                    <h4>3) Implementation and Smoothing</h4>
                    <p>The instantaneous frequency estimate is heavily smoothed to avoid rapid filter modulation.
                        Additionally, a JUCE <code>LinearSmoothedValue</code> provides exponential coefficient
                        interpolation to prevent zipper noise when updating SVF parameters. The frequency range is
                        clamped to 3 - 11 kHz to prevent tracking of low frequency transients or noise.</p>

                    <h3>F. Harmonic Excitation</h3>
                    <p>Aggressive de-essing can remove "air" and "presence", leaving vocals sounding dull. To
                        compensate, the system includes an upward expander with saturation based harmonic enchancement,
                        active only during non-sibilant regions.</p>

                    <h4>1) Algorithm</h4>
                    <p>When \( C_{excite}[n] > 0.01 \) saturation drive is:</p>
                    <div class="math-display">
                        $$ D[n] = 1 + 3 \cdot A_{\text{excite}} \cdot C_{\text{excite}}[n] $$
                    </div>
                    <p>where \( A_{excite} \) is the Excite Amount parameter between 0 - 100%.</p>
                    <p>Soft clipping is defined as:</p>
                    <div class="math-display">
                        $$ y_{\text{sat}}[n] = \tanh(D[n] \cdot x_{\text{high}}[n]) $$
                    </div>
                    <p>Normalization is defined as:</p>
                    <div class="math-display">
                        $$ y_{\text{norm}}[n] = \frac{y_{\text{sat}}[n]}{\tanh(D[n])} $$
                    </div>
                    <p>This preserves gain while introducing odd-harmonic content [11]. Amplitude boost is defined as:
                    </p>
                    <div class="math-display">
                        $$ B[n] = 10^{(9 \cdot A_{\text{excite}} \cdot C_{\text{excite}}[n])/20} $$
                    </div>
                    <p>Wet & Dry mix is defined as:</p>
                    <div class="math-display">
                        $$ W = M_{\text{excite}} \cdot C_{\text{excite}}[n] $$
                        $$ y_{\text{final}}[n] = x_{\text{high}}[n] + W \cdot (y_{\text{norm}}[n] \cdot B[n] -
                        x_{\text{high}}[n]) $$
                    </div>
                    <p>where \( M_{excite} \) is the Excite Mix parameter from 0 - 100%. As a result, non-sibilant
                        consonants and vowel transitions gain subtle harmonic richness without re-introducing harshness.
                    </p>

                    <h3>G. Lookahead Delay Compensation</h3>
                    <p>To allow the envelope follower to observe transients before they reach the processing stage, a
                        2ms circular delay buffer is implemented. This ensures the attack phase of compression is
                        without any artifacts, as the system knows about transients 2ms in advance [3].</p>
        </section>

        <section id="implementation">
            <h2>IV. Implementation</h2>

            <h3>A. JUCE Framework Architecture</h3>
            <p>The plugin and standalone application is implemented as a <code>juce::AudioProcessor</code> subclass,
                compatible with VST3 and AU formats [12].</p>

            <h4>1) Audio Processing Thread</h4>
            <p>The <code>processBlock()</code> method receives audio buffers from the DAW host or any microphone input.
            </p>

            <h4>2) Parameter Management</h4>
            <p>User controls are managed via <code>AudioProcessorValueTreeState(APVTS)</code>. Parameters are thread
                safe atomics, readable from the audio thread without locks.</p>

            <h4>3) Visualization System</h4>
            <p>Two separate visualization modules run on a refresh rate of 60 Hz on the message thread. Waveform Scope
                displays processed audio with suppression/excitation overlays. It utilizes a lock-free First In First
                Out (FIFO) to transfer data from the audio thread. The FFT Spectrum Analyzer displays frequency content
                and detector filter response. It uses JUCE's <code>dsp::FFT</code> class with Blackman-Harris windowing
                of 2048 points and 512 bin outputs [12]. The FFT is computed only for the graphics user interface (GUI)
                display and is not used in the audio processing chain.</p>
        </section>

        <section id="results">
            <h2>V. Results and Analysis</h2>

            <h3>A. Methodology</h3>
            <p>To objectively validate the system’s frequency selectivity and transient response, a comparative spectral
                analysis was conducted on a vocal recording containing pronounced sibilant fricatives (/s/, /z/). The
                primary objective was to verify that the adaptive algorithm could successfully isolate and attenuate
                sibilant energy in the time-frequency domain without degrading the fundamental vocal signal.</p>
            <p>The test utilized a male vocal recording, produced by ElevenLabs Text-To-Speech v3 GenerativeAI model,
                characterized by harsh sibilance in the 6 kHz – 11 kHz range. The plugin was configured in Auto-Freq
                mode with the Parametric mode active. The threshold was set to -18 dB with a ratio of 50%, while the
                envelope ballistics were tuned to a fast attack (2 ms) and moderate release (80 ms) to match the
                transient nature of fricative consonants.</p>

            <h3>B. Visual Data Comparison</h3>

            <h4>1) Frequency Domain</h4>
            <p>The spectral characteristics of the unprocessed input signal (Fig. 1) reveal high-intensity vertical
                bands spanning the 6 kHz to 11 kHz frequency range, corresponding to stochastic sibilant noise. Fig. 2
                illustrates the output signal after processing. A direct comparison reveals a distinct reduction in
                energy density within the identified sibilant bandwidth, indicated by the shift from bright yellow to
                darker hues. Crucially, the spectral energy below 4 kHz remains visually identical to the unprocessed
                source, demonstrating high spectral transparency.</p>

            <figure style="text-align:center; margin: 2rem 0;">
                <img src="images/sample1_off.png" alt="Unprocessed Input Signal"
                    style="max-width:100%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                <figcaption style="margin-top:0.5rem; font-style:italic; font-size:0.9rem;">Fig. 1: Unprocessed Input
                    Signal: High-intensity sibilant energy (yellow/orange) is dominant in the 6 kHz – 11 kHz range.
                </figcaption>
            </figure>

            <figure style="text-align:center; margin: 2rem 0;">
                <img src="images/sample1_on_auto.png" alt="Processed Output (Auto Mode)"
                    style="max-width:100%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                <figcaption style="margin-top:0.5rem; font-style:italic; font-size:0.9rem;">Fig. 2: Processed Output
                    (Auto Mode): Targeted attenuation of the sibilant band. Note the preservation of low-frequency
                    formants (<4 kHz) and sharp transient response.</figcaption>
            </figure>

            <h4>2) Time Domain</h4>
            <p>Fig. 3 quantifies the dynamic behavior of the processor. The upper pane displays the amplitude envelopes
                of the source (blue) and processed (orange) signals. The two envelopes are tightly aligned during
                non-sibilant periods, confirming that the processor acts as a unity-gain device when sibilance is
                absent. The lower pane of Fig. 3 isolates the instantaneous gain reduction. The calculated metrics for
                this performance window reveal a Maximum Gain Reduction of 19.75 dB occurring at the most aggressive
                sibilant peak. Conversely, the Average Gain Reduction is merely 0.67 dB across the entire sample. This
                low average value quantitatively proves the system's transparency: extreme attenuation is applied only
                during the brief sibilant instants, leaving the vast majority of the vocal performance statistically
                unaltered.</p>

            <figure style="text-align:center; margin: 2rem 0;">
                <img src="images/GR.png" alt="Gain Reduction"
                    style="max-width:100%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                <figcaption style="margin-top:0.5rem; font-style:italic; font-size:0.9rem;">Fig. 3: Gain Reduction
                    between source and processed signal</figcaption>
            </figure>

            <h3>C. Technical Interpretation</h3>
            <p>The combined data supports the validation of the adaptive tracking and dynamic processing. The
                spectrograms confirm the accuracy of the derivative-based centroid estimation, while the gain reduction
                statistics demonstrate the system's transparency during non-sibilant intervals. The variability in
                reduction depth proves that the compression ratio adapts to the signal's crest factor as defined in the
                system's power-law topology.</p>
        </section>

        <section id="discussion">
            <h2>VI. Discussion</h2>

            <h3>A. Limitations</h3>
            <p>While the proposed system demonstrates robust performance across diverse vocal material, several areas
                warrant further investigation.</p>
            <p>The derivative-based approach exhibits reduced accuracy under specific conditions:</p>
            <ul>
                <li><strong>High Noise Floors:</strong> Differentiation amplifies high-frequency noise at +6 dB/octave
                    per derivative order. When the signal-to-noise ratio (SNR) falls below 30 dB in the 6--12 kHz band,
                    spurious frequency estimates occur. This manifests as 'hunting' behavior where the SVF center
                    frequency modulates erratically. A pre-emphasis high-pass filter attenuates low-frequency noise
                    before derivative calculation. Future implementations could incorporate adaptive noise gating based
                    on spectral flatness measurements.</li>
                <li><strong>Multiband Excitation:</strong> The current excitation module operates exclusively on the
                    high band. Human hearing exhibits frequency-dependent saturation preferences: even-harmonic
                    distortion enhances perceived warmth in the 2--4 kHz "presence" region, while odd-harmonic content
                    contributes brightness above 8 kHz [13]. A multiband architecture could apply tailored saturation
                    curves across three frequency zones; low: 80-500 Hz, mid: 500-4000 Hz, high: 4000-16000 Hz.</li>
            </ul>

            <h3>B. Future Work</h3>

            <h4>1) Machine Learning-Based Phoneme Classification</h4>
            <p>The derivative-based frequency tracker makes no distinction between sibilant phonemes (/s/, /z/, /ʃ/,
                /ʒ/) and non-sibilant high-frequency content (breath noise, tongue clicks, lip smacks). Machine learning
                approaches, such as convolutional neural networks trained on phonetically annotated speech corpora,
                could potentially classify audio frames as sibilant vs. non-sibilant, reducing false positives. However,
                such approaches must maintain real-time inference latency below 5 ms to preserve system responsiveness.
            </p>

            <h4>2) Adaptive Feedback Suppression</h4>
            <p>Live sound systems suffer from feedback when microphone-captured sound re-enters the system via
                loudspeakers, typically as narrow, high-Q resonances. Automatic feedback suppression systems using
                adaptive notch filters [18] could potentially benefit from derivative-based frequency tracking
                approaches, though the faster response times required for feedback control present additional
                implementation challenges.</p>

            <h4>3) Speech Enhancement for Telecommunications</h4>
            <p>VoIP systems degrade intelligibility under low-bitrate compression and packet loss. While traditional
                speech enhancement focuses on noise reduction [15], harmonic enhancement techniques could provide an
                alternative or complementary approach for improving perceived clarity in bandwidth limited channels.</p>
        </section>

        <section id="acknowledgments">
            <h2>Acknowledgments</h2>
            <p>I sincerely want to thank my supervisor, Dr. Cumhur Erkut, for his guidance and help throughout the span
                of this work. I would also like to thank my professor, Dr. Sofia Dahl, for her valuable input.</p>
        </section>

        <section id="genai">
            <h2>Statement of Generative AI usage</h2>
            <p>Generative AI tools were utilized in this project to assist with code debugging, code completion, code
                suggestions and clarity improvements. The core content and intellectual contributions remain solely that
                of the author.</p>
            <p>The following tools were used:</p>
            <ul>
                <li><strong>Github Copilot:</strong> Supported automatic code completion in the working environment</li>
                <li><strong>OpenAI ChatGPT 5:</strong> Assisted in debugging parts of the code.</li>
                <li><strong>Google Gemini 2.5:</strong> Assisted in debugging parts of the code, implementing the
                    spectrogram
                    visualization and helped structure a sibilant rich text prompt for the ElevenLabs model.</li>
                <li><strong>ElevenLabs Text-to-Speech v3:</strong> Generated sibilant rich speech to use in the Results
                    section.</li>
            </ul>
        </section>

        <section id="references">
            <h2>References</h2>
            <ol class="references-list">
                <li>M. Senior, "Techniques For Vocal De-essing," <em>Sound on Sound</em>, May 2009. [Online]. Available:
                    <a
                        href="https://www.soundonsound.com/techniques/techniques-vocal-de-essing">https://www.soundonsound.com/techniques/techniques-vocal-de-essing</a>
                </li>
                <li>K. Linhard, P. Bulling, and A. Wolf, "Frequency Domain De-Essing for Hands-free Applications," in
                    <em>DAGA 2017 Kiel</em>, 2017. [Online]. Available: <a
                        href="https://pub.dega-akustik.de/DAGA_2017/data/articles/000071.pdf">https://pub.dega-akustik.de/DAGA_2017/data/articles/000071.pdf</a>
                </li>
                <li>W. Pirkle, <em>Designing Audio Effect Plugins in C++: For AAX, AU, and VST3 with DSP Theory</em>.
                    Routledge, 2019.</li>
                <li>X. Grandchamp, A. Van Hirtum, X. Pelorson, K. Nozaki, and S. Shimojo, "Towards Sibilant /s/
                    Modelling: Preliminary Computational Results," <em>The Journal of the Acoustical Society of
                        America</em>, vol. 123, no. 5, p. 3579, 2008.</li>
                <li>S. H. Linkwitz, "Active crossover networks for noncoincident drivers," <em>Journal of the Audio
                        Engineering Society</em>, vol. 24, no. 1, pp. 2–8, 1976.</li>
                <li>"Complete journal: volume 41 issue 11," <em>Journal of the Audio Engineering Society</em>, vol. 41,
                    no. 11, 1993.</li>
                <li>C. H. Shadle, "Articulatory-Acoustic Relationships in Fricative Consonants," in <em>Speech
                        Production and Speech Modelling</em>, W. J. Hardcastle and A. Marchal, Eds. Dordrecht: Springer
                    Netherlands, 1990, pp. 187–209.</li>
                <li>D. Jd, N. Oep, and Q. Nxwuyzr, "A large set of audio features for sound description (similarity and
                    classification) in the CUIDADO project," <em>Ircam</em>, 2004.</li>
                <li>"SPECTRAL AUDIO SIGNAL PROCESSING." [Online]. Available: <a
                        href="https://ccrma.stanford.edu/~jos/sasp/">https://ccrma.stanford.edu/~jos/sasp/</a></li>
                <li>M. Kahrs and K. Brandenburg, Eds., <em>Applications of Digital Signal Processing to Audio and
                        Acoustics</em>. Boston: Kluwer Academic Publishers, 2002.</li>
                <li>P. Dutilleux, K. Dempwolf, M. Holters, and U. Zölzer, "Nonlinear Processing," in <em>DAFX: Digital
                        Audio Effects</em>, John Wiley & Sons, Ltd, 2011, pp. 101–138.</li>
                <li>"JUCE: Main Page." [Online]. Available: <a
                        href="https://docs.juce.com/master/index.html">https://docs.juce.com/master/index.html</a></li>
                <li>P. D. Pestana and J. D. Reiss, "Intelligent audio production strategies informed by best practices,"
                    in <em>Proc. AES Int. Conf. Semantic Audio</em>, 2013.</li>
                <li>M. R. Schroeder, "Improvement of Acoustic Feedback Stability in Public Address Systems," <em>The
                        Journal of the Acoustical Society of America</em>, vol. 31, no. 6, pp. 851–852, 1959.</li>
                <li>P. C. Loizou, <em>Speech Enhancement: Theory and Practice</em>. CRC Press, 2007.</li>
                <li>U. Zölzer et al., <em>DAFX - Digital Audio Effects</em>. John Wiley & Sons, 2002.</li>
                <li>D. Giannoulis, M. Massberg, and J. Reiss, "Digital Dynamic Range Compressor Design—A Tutorial and
                    Analysis," <em>AES: Journal of the Audio Engineering Society</em>, vol. 60, 2012.</li>
                <li>B.-S. Chen, T.-Y. Yang, and B.-H. Lin, "Adaptive Notch Filter by Direct Frequency Estimation,"
                    <em>Signal Processing</em>, vol. 27, no. 2, pp. 161–176, 1992.
                </li>
            </ol>
        </section>

        <section id="parameters">
            <h2>Parameter Specifications</h2>
            <p>Table I provides a comprehensive overview of all user-adjustable parameters implemented in the plugin,
                including their valid ranges, default values, units, and functional descriptions.</p>

            <table>
                <caption>Table I: Plugin Parameter Specifications</caption>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Range</th>
                        <th>Default</th>
                        <th>Unit</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Threshold</td>
                        <td>-60 to 0</td>
                        <td>-18</td>
                        <td>dB</td>
                        <td>Relative sibilance threshold for detection</td>
                    </tr>
                    <tr>
                        <td>Ratio</td>
                        <td>0 to 100</td>
                        <td>50</td>
                        <td>%</td>
                        <td>Compression amount (0% = no reduction, 100% = maximum)</td>
                    </tr>
                    <tr>
                        <td>Attack</td>
                        <td>0.05 to 20</td>
                        <td>2</td>
                        <td>ms</td>
                        <td>Envelope follower attack time constant</td>
                    </tr>
                    <tr>
                        <td>Release</td>
                        <td>5 to 300</td>
                        <td>80</td>
                        <td>ms</td>
                        <td>Envelope follower release time constant</td>
                    </tr>
                    <tr>
                        <td>Detect Freq</td>
                        <td>3000 to 10000</td>
                        <td>6000</td>
                        <td>Hz</td>
                        <td>SVF bandpass center frequency for sibilance isolation</td>
                    </tr>
                    <tr>
                        <td>Q Factor</td>
                        <td>0.4 to 5.0</td>
                        <td>2.0</td>
                        <td>---</td>
                        <td>Filter resonance (bandwidth control)</td>
                    </tr>
                    <tr>
                        <td>Split Freq</td>
                        <td>3000 to 10000</td>
                        <td>6000</td>
                        <td>Hz</td>
                        <td>Crossover frequency for split-band processing</td>
                    </tr>
                    <tr>
                        <td>Suppress Mix</td>
                        <td>0 to 100</td>
                        <td>100</td>
                        <td>%</td>
                        <td>Wet/dry blend for gain reduction (0% = bypass)</td>
                    </tr>
                    <tr>
                        <td>Excite Amount</td>
                        <td>0 to 100</td>
                        <td>35</td>
                        <td>%</td>
                        <td>Harmonic saturation drive intensity</td>
                    </tr>
                    <tr>
                        <td>Excite Mix</td>
                        <td>0 to 100</td>
                        <td>40</td>
                        <td>%</td>
                        <td>Exciter wet/dry blend</td>
                    </tr>
                    <tr>
                        <td>Output Gain</td>
                        <td>-24 to +24</td>
                        <td>0</td>
                        <td>dB</td>
                        <td>Makeup gain applied to processed signal</td>
                    </tr>
                    <tr>
                        <td>Mode</td>
                        <td>1, 2, 3</td>
                        <td>3</td>
                        <td>---</td>
                        <td>Processing topology (1=Split-Band, 2=Wideband, 3=Parametric)</td>
                    </tr>
                    <tr>
                        <td>Auto Freq</td>
                        <td>Off/On</td>
                        <td>Off</td>
                        <td>---</td>
                        <td>Enable adaptive frequency tracking</td>
                    </tr>
                    <tr>
                        <td>Listen</td>
                        <td>Off/On</td>
                        <td>Off</td>
                        <td>---</td>
                        <td>Solo sidechain bandpass signal (monitoring mode)</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="code-availability">
            <h2>Code Availability</h2>
            <p>The source code, plugin binaries, and project documentation are available online.</p>

            <p><strong>GitHub Repository:</strong><br>
                <a
                    href="https://github.com/stefbil/Adaptive-Media-Systems-Adaptive-DeEsser">https://github.com/stefbil/Adaptive-Media-Systems-Adaptive-DeEsser</a>
            </p>

            <p><strong>Project Webpage:</strong><br>
                <a href="https://stefbil.github.io/stefbilmedams">https://stefbil.github.io/stefbilmedams</a>
            </p>

            <p><strong>Project Video:</strong></p>
            <div
                style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-top: 0.5rem; margin-bottom: 0.5rem;">
                <iframe src="https://www.youtube-nocookie.com/embed/hmta5HOfDE4?si=Fg1BCJnAkpthxpPd"
                    title="YouTube video player"
                    frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    referrerpolicy="strict-origin-when-cross-origin" allowfullscreen
                    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
                    </iframe>
            </div>
        </section>

    </main>

    <footer>
        <p>&copy; 2025 Stefanos Biliousis. Adaptive Deesser Application.</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const sections = document.querySelectorAll('section');
            const navLinks = document.querySelectorAll('nav a');

            const observerOptions = {
                root: null,
                rootMargin: '-20% 0px -50% 0px', // Active when 20% from top to 50% from bottom
                threshold: 0
            };

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        // Remove active class from all links
                        navLinks.forEach(link => link.classList.remove('active'));

                        // Add active class to corresponding link
                        const id = entry.target.getAttribute('id');
                        const activeLink = document.querySelector(`nav a[href="#${id}"]`);
                        if (activeLink) {
                            activeLink.classList.add('active');
                        }
                    }
                });
            }, observerOptions);

            sections.forEach(section => {
                observer.observe(section);
            });
        });
    </script>

</body>


</html>

